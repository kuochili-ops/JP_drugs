import streamlit as st
import pdfplumber
import pandas as pd
import requests
import re
import time
import io
from urllib.parse import quote
from bs4 import BeautifulSoup

# --- 1. è¨­å®šå€åŸŸ (Azure æ†‘æ“š) ---
AZURE_KEY = "9JDF24qrsW8rXiYmChS17yEPyNRI96nNXXqEKn5CyI6ql6iYcTOFJQQJ99BLAC3pKaRXJ3w3AAAbACOGVYVU"
AZURE_ENDPOINT = "https://api.cognitive.microsofttranslator.com"
AZURE_REGION = "eastasia"

# --- 2. æ ¸å¿ƒå°ç…§é‚è¼¯ï¼šAzure å„ªå…ˆ â” KEGG è£œåº• ---
def get_english_name_strict(jp_name):
    """
    åš´æ ¼é‚è¼¯ï¼š
    1. å˜—è©¦ Azure ç¿»è­¯
    2. è‹¥ Azure å›å‚³ç‚ºç©ºæˆ–å‡ºéŒ¯ï¼Œå‰‡å•Ÿå‹• KEGG çˆ¬èŸ²
    """
    if not jp_name or len(str(jp_name)) < 2:
        return "N/A", "Skip"

    # æ¸…ç†æ—¥æ–‡åç¨±ï¼ˆç§»é™¤åŠ‘å‹æ‹¬è™Ÿï¼‰
    clean_ja = re.split(r'[\(\n\sï¼ˆ]', str(jp_name))[0].strip()
    
    # --- Step 1: Azure Translator ---
    try:
        url = f"{AZURE_ENDPOINT.strip('/')}/translate?api-version=3.0&from=ja&to=en"
        headers = {
            'Ocp-Apim-Subscription-Key': AZURE_KEY,
            'Ocp-Apim-Subscription-Region': AZURE_REGION,
            'Content-type': 'application/json; charset=utf-8'
        }
        res = requests.post(url, headers=headers, json=[{'text': clean_ja}], timeout=8)
        if res.status_code == 200:
            en_result = res.json()[0]['translations'][0]['text']
            if en_result and len(en_result) > 2:
                return en_result, "Azure"
    except:
        pass

    # --- Step 2: KEGG Medicus (ç•¶ Azure å¤±æ•—æ™‚) ---
    try:
        search_url = f"https://www.kegg.jp/medicus-bin/search_drug?search_keyword={quote(clean_ja)}"
        r_s = requests.get(search_url, timeout=10)
        codes = re.findall(r'japic_code=(\d+)', r_s.text + r_s.url)
        if codes:
            jid = codes[0].zfill(8)
            ri = requests.get(f"https://www.kegg.jp/medicus-bin/japic_med?id={jid}")
            ri.encoding = ri.apparent_encoding
            soup = BeautifulSoup(ri.text, 'html.parser')
            th = soup.find('th', string=re.compile(r'æ¬§æ–‡ä¸€èˆ¬å'))
            if th and th.find_next_sibling('td'):
                return th.find_next_sibling('td').get_text(strip=True), "KEGG"
    except:
        pass

    return "[å°ç…§å¤±æ•—]", "None"

# --- 3. éŒ¨é»å®šæ¨™è§£æå‡½å¼ (å·²æ ¡æº–è‡³ 505+ é …) ---
def parse_pdf_with_anchors(file):
    all_data = []
    current_cat = "æœªçŸ¥é¡åˆ¥"
    
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            # é¡åˆ¥åˆ¤å®š
            if "(1)" in text: current_cat = "Cat A (æœ€å„ªå…ˆ)"
            elif "(2)" in text: current_cat = "Cat B (å„ªå…ˆ)"
            elif "(3)" in text: current_cat = "Cat C (ç©©å®šç¢ºä¿)"

            lines = text.split('\n')
            for line in lines:
                line = line.strip()
                if not line or "æˆåˆ†å" in line: continue
                
                # æ‚¨çš„éŒ¨é»é‚è¼¯ï¼š(çµ¦è—¥æ–¹å¼) (3ä½ç”¨é€”ç·¨è™Ÿ) (æˆåˆ†å)
                match = re.search(r'^(å†…|æ³¨|å¤–)\s*(\d{3})\s*(.+)$', line)
                
                if match:
                    route, code, name = match.groups()
                    all_data.append({
                        "é¡åˆ¥": current_cat,
                        "çµ¦è—¥æ–¹å¼": route,
                        "ç”¨é€”é¡åˆ¥": code,
                        "æˆåˆ†æ—¥æ–‡å": name.strip()
                    })
                else:
                    # è·¨è¡Œåˆä½µé‚è¼¯ï¼šå°‡æ–·è¡Œçš„è—¥åæ¥å›ä¸Šä¸€ç­†
                    if all_data and not re.match(r'^\d+$', line):
                        if len(line) > 1 and "åšç”ŸåŠ´åƒçœ" not in line:
                            all_data[-1]["æˆåˆ†æ—¥æ–‡å"] += line.strip()

    # æ¸…æ´—æ•¸æ“š
    for item in all_data:
        # ç§»é™¤å¯èƒ½å¤¾é›œçš„é›œè¨Š
        item["æˆåˆ†æ—¥æ–‡å"] = re.sub(r'\s+', '', item["æˆåˆ†æ—¥æ–‡å"])
        item["æˆåˆ†æ—¥æ–‡å"] = re.sub(r'\d+$', '', item["æˆåˆ†æ—¥æ–‡å"])
    
    return pd.DataFrame(all_data)

# --- 4. Streamlit UI ä»‹é¢ ---
st.set_page_config(layout="wide", page_title="è—¥å“ 506 é …å…¨è§£æ")
st.title("ğŸ’Š å®‰å®šç¢ºä¿é†«è—¥å“å…¨é‡è§£æ (Azure å„ªå…ˆç‰ˆ)")
st.info("ç•¶å‰é‚è¼¯ï¼šéŒ¨é»å®šæ¨™æƒæ â” Azure ç¿»è­¯ â” KEGG è£œåº•å°ç…§")

f = st.file_uploader("ä¸Šå‚³ PDF (000785498.pdf)", type=['pdf'])

if f:
    if 'final_df' not in st.session_state:
        st.session_state.raw_list = parse_pdf_with_anchors(f)
    
    df = st.session_state.raw_list
    st.success(f"âœ… æˆåŠŸæå– {len(df)} é …æˆåˆ†ï¼")
    st.dataframe(df, use_container_width=True)

    if st.button("ğŸš€ é–‹å§‹åŸ·è¡Œå…¨é‡ 505 é …ç¿»è­¯å°ç…§"):
        results = []
        bar = st.progress(0)
        status = st.empty()
        
        for i, row in df.iterrows():
            jp_name = row["æˆåˆ†æ—¥æ–‡å"]
            status.text(f"æ­£åœ¨å°ç…§ ({i+1}/{len(df)}): {jp_name}")
            
            # åŸ·è¡Œåš´æ ¼å°ç…§é‚è¼¯
            en_name, source = get_english_name_strict(jp_name)
            
            results.append({
                "é¡åˆ¥": row["é¡åˆ¥"],
                "çµ¦è—¥æ–¹å¼": row["çµ¦è—¥æ–¹å¼"],
                "ç”¨é€”é¡åˆ¥": row["ç”¨é€”é¡åˆ¥"],
                "æˆåˆ†æ—¥æ–‡å": jp_name,
                "æˆåˆ†è‹±æ–‡å": en_name,
                "å°ç…§ä¾†æº": source
            })
            bar.progress((i + 1) / len(df))
            if i % 15 == 0: time.sleep(0.1)
            
        st.session_state.result_df = pd.DataFrame(results)
        st.success("ğŸ‰ å…¨é‡å°ç…§å®Œæˆï¼")
        st.dataframe(st.session_state.result_df, use_container_width=True)
        
        # ä¸‹è¼‰ Excel
        out = io.BytesIO()
        with pd.ExcelWriter(out, engine='xlsxwriter') as writer:
            st.session_state.result_df.to_excel(writer, index=False)
        st.download_button("ğŸ“¥ ä¸‹è¼‰å…¨è§£æå ±å‘Š (Excel)", out.getvalue(), "Medicine_Full_Report.xlsx")
