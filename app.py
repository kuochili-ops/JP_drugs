import streamlit as st
import pdfplumber
import pandas as pd
import requests
import re
import time
import io
from urllib.parse import quote
from bs4 import BeautifulSoup

# --- 1. è¨­å®šå€åŸŸ (Azure ç¿»è­¯) ---
AZURE_KEY = "9JDF24qrsW8rXiYmChS17yEPyNRI96nNXXqEKn5CyI6ql6iYcTOFJQQJ99BLAC3pKaRXJ3w3AAAbACOGVYVU"
AZURE_ENDPOINT = "https://api.cognitive.microsofttranslator.com"
AZURE_REGION = "eastasia"

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def fetch_japic_en_name(jp_name):
    """
    æ²¿ç”¨æ‚¨çš„é‚è¼¯ï¼šå¾ KEGG/Japic æŠ“å–è‹±æ–‡æˆåˆ†å
    """
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64) AppleWebKit/537.36"}
    # æ¸…ç†æ—¥æ–‡åç¨±ï¼Œåªå–æ ¸å¿ƒç‰‡å‡å
    raw_clean = re.split(r'[\(\n\sï¼ˆ]', str(jp_name))[0].strip()
    katakana_match = re.match(r'^[\u30A0-\u30FF\u30FB\u30FC]+', raw_clean)
    search_keyword = katakana_match.group(0) if katakana_match else raw_clean
    
    if len(search_keyword) < 2: return "[æ ¼å¼ä¸ç¬¦]"

    try:
        search_url = f"https://www.kegg.jp/medicus-bin/search_drug?search_keyword={quote(search_keyword)}"
        r_s = requests.get(search_url, headers=headers, timeout=10)
        codes = re.findall(r'japic_code=(\d+)', r_s.text + r_s.url)
        
        if codes:
            jid = codes[0].zfill(8)
            ri = requests.get(f"https://www.kegg.jp/medicus-bin/japic_med?japic_code={jid}", headers=headers)
            ri.encoding = ri.apparent_encoding
            i_soup = BeautifulSoup(ri.text, 'html.parser')
            th = i_soup.find('th', string=re.compile(r'æ¬§æ–‡ä¸€èˆ¬å'))
            if th and th.find_next_sibling('td'):
                return th.find_next_sibling('td').get_text(strip=True)
    except: pass
    return "[æœªæª¢å‡º]"

def parse_medicine_pdf(file):
    """
    è§£æ PDF è¡¨æ ¼ä¸¦æå–ï¼šçµ¦è—¥æ–¹å¼ã€ç”¨é€”é¡åˆ¥ã€æˆåˆ†å
    """
    all_rows = []
    current_cat = "æœªçŸ¥é¡åˆ¥"
    
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            # é¡åˆ¥åˆ¤å®šé‚è¼¯
            if "(1)" in text or "ã‚«ãƒ†ã‚´ãƒªA" in text: current_cat = "Category A (æœ€å„ªå…ˆ)"
            elif "(2)" in text or "ã‚«ãƒ†ã‚´ãƒªB" in text: current_cat = "Category B (å„ªå…ˆ)"
            elif "(3)" in text or "ã‚«ãƒ†ã‚´ãƒªC" in text: current_cat = "Category C (ä¸€èˆ¬)"

            tables = page.extract_tables()
            for table in tables:
                for row in table:
                    # æ ¹æ“š PDF çµæ§‹ï¼Œrow[0]=çµ¦è—¥æ–¹å¼, row[1]=ç·¨è™Ÿ, row[2]=æ—¥æ–‡å
                    if not row or len(row) < 3: continue
                    route = str(row[0]).strip().replace('\n', '')
                    class_no = str(row[1]).strip().replace('\n', '')
                    name_jp = str(row[2]).strip().replace('\n', ' ')

                    # éæ¿¾æœ‰æ•ˆè³‡æ–™è¡Œï¼ˆçµ¦è—¥æ–¹å¼é€šå¸¸ç‚º å…§ã€æ³¨ã€å¤–ï¼‰
                    if route in ['å†…', 'æ³¨', 'å¤–']:
                        all_rows.append({
                            "é¡åˆ¥": current_cat,
                            "çµ¦è—¥æ–¹å¼": route,
                            "ç”¨é€”é¡åˆ¥ (ç·¨è™Ÿ)": class_no,
                            "æˆåˆ†æ—¥æ–‡å": name_jp
                        })
    return pd.DataFrame(all_rows)

# --- 3. Streamlit UI ---

st.set_page_config(layout="wide", page_title="æ—¥æœ¬å®‰å®šç¢ºä¿é†«è—¥å“å°ç…§å·¥å…·")
st.title("ğŸ’Š æ—¥æœ¬å®‰å®šç¢ºä¿é†«è—¥å“è§£æå·¥å…·")
st.info("ä¸Šå‚³åšå‹çœ PDFï¼Œç³»çµ±å°‡è‡ªå‹•è§£æè¡¨æ ¼ä¸¦é€é KEGG æŠ“å–è‹±æ–‡æˆåˆ†åã€‚")

uploaded_file = st.file_uploader("è«‹ä¸Šå‚³å®‰å®šç¢ºä¿é†«è—¥å“ PDF æª”æ¡ˆ", type=['pdf'])

if uploaded_file:
    if st.button("é–‹å§‹è§£æä¸¦å°ç…§è‹±æ–‡å"):
        # ç¬¬ä¸€æ­¥ï¼šè§£æ PDF
        with st.spinner("æ­£åœ¨è®€å– PDF è¡¨æ ¼..."):
            df = parse_medicine_pdf(uploaded_file)
        
        if not df.empty:
            st.success(f"æˆåŠŸè§£æ {len(df)} é …è—¥å“ï¼Œé–‹å§‹é€²è¡Œ KEGG è‹±æ–‡åå°ç…§...")
            
            # ç¬¬äºŒæ­¥ï¼šå°ç…§è‹±æ–‡å
            results = []
            progress_bar = st.progress(0)
            
            for i, row in df.iterrows():
                # å‘¼å«æ‚¨æä¾›çš„å°ç…§é‚è¼¯
                en_name = fetch_japic_en_name(row["æˆåˆ†æ—¥æ–‡å"])
                
                results.append({
                    "é¡åˆ¥": row["é¡åˆ¥"],
                    "çµ¦è—¥æ–¹å¼": row["çµ¦è—¥æ–¹å¼"],
                    "ç”¨é€”é¡åˆ¥": row["ç”¨é€”é¡åˆ¥ (ç·¨è™Ÿ)"],
                    "æˆåˆ†æ—¥æ–‡å": row["æˆåˆ†æ—¥æ–‡å"],
                    "æˆåˆ†è‹±æ–‡å": en_name
                })
                
                # æ›´æ–°é€²åº¦æ¢
                progress_bar.progress((i + 1) / len(df))
                # é¿å…é »ç¹è«‹æ±‚è¢«å°é–
                if i % 5 == 0: time.sleep(0.2)
            
            final_df = pd.DataFrame(results)
            st.dataframe(final_df, use_container_width=True)
            
            # ä¸‹è¼‰æˆæœ
            csv = final_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ“¥ ä¸‹è¼‰å°ç…§åˆ—è¡¨ (CSV)", csv, "Japan_Medicine_List.csv", "text/csv")
        else:
            st.error("æœªèƒ½è­˜åˆ¥ PDF ä¸­çš„è¡¨æ ¼å…§å®¹ã€‚")
