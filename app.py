import streamlit as st
import pdfplumber
import pandas as pd
import requests
import re
import time
import io
from urllib.parse import quote
from bs4 import BeautifulSoup

# --- 1. è¨­å®šå€åŸŸ ---
AZURE_KEY = "9JDF24qrsW8rXiYmChS17yEPyNRI96nNXXqEKn5CyI6ql6iYcTOFJQQJ99BLAC3pKaRXJ3w3AAAbACOGVYVU"
AZURE_ENDPOINT = "https://api.cognitive.microsofttranslator.com"
AZURE_REGION = "eastasia"

# --- 2. åŠŸèƒ½å‡½å¼åº« ---

def translate_via_azure(text):
    """ ç¬¬ä¸€éšæ®µï¼šå˜—è©¦ä½¿ç”¨ Azure ç¿»è­¯æˆåˆ†å """
    if not text: return ""
    clean_text = str(text).split('(')[0].split('ï¼ˆ')[0].strip() # ç§»é™¤åŠ‘å‹æ‹¬è™Ÿä»¥åˆ©ç¿»è­¯
    url = f"{AZURE_ENDPOINT.strip('/')}/translate?api-version=3.0&from=ja&to=en"
    headers = {
        'Ocp-Apim-Subscription-Key': AZURE_KEY,
        'Ocp-Apim-Subscription-Region': AZURE_REGION,
        'Content-type': 'application/json; charset=utf-8'
    }
    try:
        response = requests.post(url, headers=headers, json=[{'text': clean_text}], timeout=10)
        if response.status_code == 200:
            res_data = response.json()
            return res_data[0]['translations'][0]['text']
    except: pass
    return None

def fetch_from_kegg(jp_name):
    """ ç¬¬äºŒéšæ®µï¼šAzure å¤±æ•—æˆ–éœ€è¦ç²¾ç¢ºè¡“èªæ™‚ï¼ŒæŸ¥è©¢ KEGG """
    headers = {"User-Agent": "Mozilla/5.0"}
    raw_clean = re.split(r'[\(\n\sï¼ˆ]', str(jp_name))[0].strip()
    try:
        search_url = f"https://www.kegg.jp/medicus-bin/search_drug?search_keyword={quote(raw_clean)}"
        r_s = requests.get(search_url, headers=headers, timeout=10)
        codes = re.findall(r'japic_code=(\d+)', r_s.text + r_s.url)
        if codes:
            jid = codes[0].zfill(8)
            ri = requests.get(f"https://www.kegg.jp/medicus-bin/japic_med?japic_code={jid}", headers=headers)
            ri.encoding = ri.apparent_encoding
            i_soup = BeautifulSoup(ri.text, 'html.parser')
            th = i_soup.find('th', string=re.compile(r'æ¬§æ–‡ä¸€èˆ¬å'))
            if th and th.find_next_sibling('td'):
                return th.find_next_sibling('td').get_text(strip=True)
    except: pass
    return "[å°ç…§å¤±æ•—]"

def parse_medicine_pdf(file):
    """ è§£æ PDF ä¸¦æå–åŸºæœ¬æ¬„ä½ """
    data = []
    current_cat = "æœªçŸ¥"
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            if "(1)" in text: current_cat = "ã‚«ãƒ†ã‚´ãƒª A"
            elif "(2)" in text: current_cat = "ã‚«ãƒ†ã‚´ãƒª B"
            elif "(3)" in text: current_cat = "ã‚«ãƒ†ã‚´ãƒª C"
            
            tables = page.extract_tables()
            for table in tables:
                for row in table:
                    if len(row) >= 3 and str(row[0]).strip() in ['å†…', 'æ³¨', 'å¤–', 'æ³¨ æ³¨']:
                        data.append({
                            "é¡åˆ¥": current_cat,
                            "çµ¦è—¥æ–¹å¼": str(row[0]).replace('\n', ' ').strip(),
                            "ç”¨é€”é¡åˆ¥": str(row[1]).strip(),
                            "æˆåˆ†æ—¥æ–‡å": str(row[2]).strip().replace('\n', '')
                        })
    return pd.DataFrame(data)

# --- 3. Streamlit ä»‹é¢ ---
st.title("ğŸ’Š å®‰å®šç¢ºä¿é†«è—¥å“å…¨é‡å°ç…§ç³»çµ±")
st.write("é‚è¼¯ï¼šAzure ç¿»è­¯å„ªå…ˆ â” KEGG è³‡æ–™åº«è£œåº•")

f = st.file_uploader("ä¸Šå‚³ PDF æª”æ¡ˆ", type=['pdf'])

if f:
    if 'raw_df' not in st.session_state:
        st.session_state.raw_df = parse_medicine_pdf(f)
    
    df = st.session_state.raw_df
    st.write(f"å·²è®€å– {len(df)} é …æˆåˆ†ã€‚")
    
    if st.button("é–‹å§‹ 506 é …å…¨è§£æ"):
        results = []
        bar = st.progress(0)
        msg = st.empty()
        
        for i, row in df.iterrows():
            jp_name = row["æˆåˆ†æ—¥æ–‡å"]
            msg.text(f"æ­£åœ¨è™•ç† ({i+1}/{len(df)}): {jp_name}")
            
            # ç­–ç•¥å¯¦æ–½ï¼šå…ˆçœ‹ Azure
            en_name = translate_via_azure(jp_name)
            source = "Azure Translator"
            
            # å¦‚æœ Azure æ²’çµæœï¼Œå» KEGG æ‰¾
            if not en_name or "[API éŒ¯èª¤" in en_name:
                en_name = fetch_from_kegg(jp_name)
                source = "KEGG/Japic"
            
            results.append({
                "é¡åˆ¥": row["é¡åˆ¥"],
                "çµ¦è—¥æ–¹å¼": row["çµ¦è—¥æ–¹å¼"],
                "ç”¨é€”é¡åˆ¥": row["ç”¨é€”é¡åˆ¥"],
                "æˆåˆ†æ—¥æ–‡å": jp_name,
                "æˆåˆ†è‹±æ–‡å": en_name,
                "è³‡æ–™ä¾†æº": source
            })
            bar.progress((i + 1) / len(df))
            if i % 10 == 0: time.sleep(0.1) # ç·©è¡
            
        final_df = pd.DataFrame(results)
        st.success("è§£æå®Œæˆï¼")
        st.dataframe(final_df)
        
        # ä¸‹è¼‰ Excel
        out = io.BytesIO()
        with pd.ExcelWriter(out, engine='xlsxwriter') as writer:
            final_df.to_excel(writer, index=False)
        st.download_button("ğŸ“¥ ä¸‹è¼‰å…¨è§£æå ±å‘Š", out.getvalue(), "Medicine_Report.xlsx")
