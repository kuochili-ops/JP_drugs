import streamlit as st
import pdfplumber
import pandas as pd
import requests
import re
import time
import io
from urllib.parse import quote
from bs4 import BeautifulSoup

# --- 1. è¨­å®šå€åŸŸ (è«‹ç¢ºä¿ Key æ­£ç¢º) ---
AZURE_KEY = "9JDF24qrsW8rXiYmChS17yEPyNRI96nNXXqEKn5CyI6ql6iYcTOFJQQJ99BLAC3pKaRXJ3w3AAAbACOGVYVU"
AZURE_ENDPOINT = "https://api.cognitive.microsofttranslator.com"
AZURE_REGION = "eastasia"

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def translate_via_azure(text):
    """ ç¬¬ä¸€éšæ®µï¼šä½¿ç”¨ Azure ç¿»è­¯æˆåˆ†å """
    if not text or len(str(text)) < 2: return None
    # æ¸…ç†åç¨±ï¼šç§»é™¤æ‹¬è™Ÿå‚™è¨»ä»¥åˆ©ç¿»è­¯
    clean_text = re.split(r'[\(\n\sï¼ˆ]', str(text))[0].strip()
    url = f"{AZURE_ENDPOINT.strip('/')}/translate?api-version=3.0&from=ja&to=en"
    headers = {
        'Ocp-Apim-Subscription-Key': AZURE_KEY,
        'Ocp-Apim-Subscription-Region': AZURE_REGION,
        'Content-type': 'application/json; charset=utf-8'
    }
    try:
        response = requests.post(url, headers=headers, json=[{'text': clean_text}], timeout=10)
        if response.status_code == 200:
            return response.json()[0]['translations'][0]['text']
    except: pass
    return None

def fetch_from_kegg(jp_name):
    """ ç¬¬äºŒéšæ®µï¼šAzure å¤±æ•—æ™‚ï¼ŒæŸ¥è©¢ KEGG è³‡æ–™åº« """
    headers = {"User-Agent": "Mozilla/5.0"}
    raw_clean = re.split(r'[\(\n\sï¼ˆ]', str(jp_name))[0].strip()
    try:
        search_url = f"https://www.kegg.jp/medicus-bin/search_drug?search_keyword={quote(raw_clean)}"
        r_s = requests.get(search_url, headers=headers, timeout=10)
        codes = re.findall(r'japic_code=(\d+)', r_s.text + r_s.url)
        if codes:
            jid = codes[0].zfill(8)
            ri = requests.get(f"https://www.kegg.jp/medicus-bin/japic_med?japic_code={jid}", headers=headers)
            ri.encoding = ri.apparent_encoding
            i_soup = BeautifulSoup(ri.text, 'html.parser')
            th = i_soup.find('th', string=re.compile(r'æ¬§æ–‡ä¸€èˆ¬å'))
            if th and th.find_next_sibling('td'):
                return th.find_next_sibling('td').get_text(strip=True)
    except: pass
    return "[å°ç…§å¤±æ•—]"

def parse_full_pdf(file):
    """ è§£æ 506 é … PDFï¼šå…¼å®¹è¡¨æ ¼èˆ‡ç´”æ–‡å­—æ¨¡å¼ """
    all_data = []
    current_cat = "æœªçŸ¥"
    
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            # å‹•æ…‹åˆ¤å®šé¡åˆ¥
            if "(1)" in text: current_cat = "ã‚«ãƒ†ã‚´ãƒª A (æœ€å„ªå…ˆ)"
            elif "(2)" in text: current_cat = "ã‚«ãƒ†ã‚´ãƒª B (å„ªå…ˆ)"
            elif "(3)" in text: current_cat = "ã‚«ãƒ†ã‚´ãƒª C (å®‰å®šç¢ºä¿)"

            # A. è™•ç†è¡¨æ ¼æ¨¡å¼ (ä¸»è¦é‡å°å‰10é )
            tables = page.extract_tables()
            for table in tables:
                for row in table:
                    if not row or len(row) < 3: continue
                    route = str(row[0]).strip()
                    # è™•ç† "æ³¨ æ³¨" æˆ–æ›è¡Œé‡ç–Šæƒ…æ³
                    if any(r in route for r in ['å†…', 'æ³¨', 'å¤–']):
                        clean_route = "".join(sorted(list(set(re.findall(r'å†…|æ³¨|å¤–', route)))))
                        all_data.append({
                            "é¡åˆ¥": current_cat,
                            "çµ¦è—¥æ–¹å¼": clean_route,
                            "ç”¨é€”é¡åˆ¥": str(row[1]).strip().split('\n')[0],
                            "æˆåˆ†æ—¥æ–‡å": str(row[2]).strip().replace('\n', '')
                        })

            # B. è™•ç†ç´”æ–‡å­—æ¨¡å¼ (è£œè¶³ç¬¬11é å¾Œçš„å…§å®¹)
            lines = text.split('\n')
            for line in lines:
                # åŒ¹é…æ ¼å¼å¦‚: "æ³¨ 311 ãƒã‚­ã‚µã‚«ãƒ«ã‚·ãƒˆãƒ¼ãƒ«"
                match = re.search(r'^(å†…|æ³¨|å¤–)\s+(\d{3})\s+(.+)$', line.strip())
                if match:
                    route, cat_no, name = match.groups()
                    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ (é¿å…èˆ‡è¡¨æ ¼é‡è¤‡æŠ“å–)
                    if not any(d['æˆåˆ†æ—¥æ–‡å'] == name for d in all_data):
                        all_data.append({
                            "é¡åˆ¥": current_cat,
                            "çµ¦è—¥æ–¹å¼": route,
                            "ç”¨é€”é¡åˆ¥": cat_no,
                            "æˆåˆ†æ—¥æ–‡å": name
                        })
    return pd.DataFrame(all_data)

# --- 3. Streamlit UI ---
st.set_page_config(layout="wide", page_title="å®‰å®šç¢ºä¿é†«è—¥å“ 506 é …å…¨è§£æ")
st.title("ğŸ’Š å®‰å®šç¢ºä¿é†«è—¥å“å…¨é‡å°ç…§ç³»çµ± (506é …)")
st.info("è§£æç­–ç•¥ï¼šAzure ç¿»è­¯å„ªå…ˆ â” KEGG è³‡æ–™åº«è£œåº• â” æ”¯æ´æ–‡å­—èˆ‡è¡¨æ ¼æ··åˆ PDF")

f = st.file_uploader("è«‹ä¸Šå‚³ 000785498.pdf", type=['pdf'])

if f:
    if 'data_list' not in st.session_state:
        with st.spinner("ç¬¬ä¸€éšæ®µï¼šæ­£åœ¨å¾ PDF æå– 506 é …æ¸…å–®..."):
            st.session_state.data_list = parse_full_pdf(f)
    
    df = st.session_state.data_list
    st.write(f"âœ… æˆåŠŸè®€å–æ¸…å–®ï¼Œå…±è¨ˆ {len(df)} é …æˆåˆ†ã€‚")
    st.dataframe(df, use_container_width=True)

    if st.button("ğŸš€ é–‹å§‹å…¨é‡å°ç…§è‹±æ–‡æˆåˆ†å"):
        results = []
        bar = st.progress(0)
        status = st.empty()
        
        for i, row in df.iterrows():
            jp_name = row["æˆåˆ†æ—¥æ–‡å"]
            status.text(f"æ­£åœ¨è™•ç† ({i+1}/{len(df)}): {jp_name}")
            
            # å„ªå…ˆ Azure
            en_name = translate_via_azure(jp_name)
            source = "Azure Translator"
            
            # å¤±æ•—å‰‡è½‰å‘ KEGG
            if not en_name:
                en_name = fetch_from_kegg(jp_name)
                source = "KEGG/Japic"
            
            results.append({
                "é¡åˆ¥": row["é¡åˆ¥"],
                "çµ¦è—¥æ–¹å¼": row["çµ¦è—¥æ–¹å¼"],
                "ç”¨é€”é¡åˆ¥": row["ç”¨é€”é¡åˆ¥"],
                "æˆåˆ†æ—¥æ–‡å": jp_name,
                "æˆåˆ†è‹±æ–‡å": en_name,
                "è³‡æ–™ä¾†æº": source
            })
            bar.progress((i + 1) / len(df))
            if i % 15 == 0: time.sleep(0.2) # é˜²æ­¢éå¿«
            
        final_df = pd.DataFrame(results)
        st.session_state.final_df = final_df
        st.success("ğŸ‰ 506 é …å…¨è§£æå°ç…§å®Œæˆï¼")
        st.dataframe(final_df, use_container_width=True)
        
        # ä¸‹è¼‰ Excel
        out = io.BytesIO()
        with pd.ExcelWriter(out, engine='xlsxwriter') as writer:
            final_df.to_excel(writer, index=False)
        st.download_button("ğŸ“¥ ä¸‹è¼‰å…¨è§£æå ±å‘Š (Excel)", out.getvalue(), "Medicine_Full_Report.xlsx")
