import streamlit as st
import pdfplumber
import pandas as pd
import requests
import re
import time
import io
from urllib.parse import quote
from bs4 import BeautifulSoup

# --- 1. Azure è¨­å®š ---
AZURE_KEY = "9JDF24qrsW8rXiYmChS17yEPyNRI96nNXXqEKn5CyI6ql6iYcTOFJQQJ99BLAC3pKaRXJ3w3AAAbACOGVYVU"
AZURE_ENDPOINT = "https://api.cognitive.microsofttranslator.com"
AZURE_REGION = "eastasia"

# --- 2. ç¿»è­¯èˆ‡å°ç…§é‚è¼¯ (åš´æ ¼åŸ·è¡Œå…ˆå¾Œé †åº) ---

def get_english_name(jp_name):
    """
    æ ¸å¿ƒé‚è¼¯ï¼š
    1. å…ˆå˜—è©¦ Azure ç¿»è­¯
    2. å¦‚æœ Azure å¤±æ•— (None æˆ–éŒ¯èª¤)ï¼Œå†çˆ¬ KEGG
    """
    if not jp_name or str(jp_name).lower() == 'none':
        return "N/A", "Skip"

    # --- Step 1: Azure ç¿»è­¯ ---
    en_name = None
    try:
        # æ¸…ç†æ—¥æ–‡ï¼Œç§»é™¤æ‹¬è™Ÿ
        clean_ja = re.split(r'[\(\n\sï¼ˆ]', str(jp_name))[0].strip()
        url = f"{AZURE_ENDPOINT.strip('/')}/translate?api-version=3.0&from=ja&to=en"
        headers = {
            'Ocp-Apim-Subscription-Key': AZURE_KEY,
            'Ocp-Apim-Subscription-Region': AZURE_REGION,
            'Content-type': 'application/json; charset=utf-8'
        }
        res = requests.post(url, headers=headers, json=[{'text': clean_ja}], timeout=8)
        if res.status_code == 200:
            en_name = res.json()[0]['translations'][0]['text']
            # å¦‚æœç¿»è­¯çµæœçœ‹èµ·ä¾†æœ‰æ•ˆï¼Œç›´æ¥å›å‚³
            if en_name and len(en_name) > 2:
                return en_name, "Azure"
    except:
        pass

    # --- Step 2: KEGG çˆ¬èŸ² (ç•¶ Azure å¤±æ•—æ™‚) ---
    try:
        search_keyword = re.split(r'[\(\n\sï¼ˆ]', str(jp_name))[0].strip()
        search_url = f"https://www.kegg.jp/medicus-bin/search_drug?search_keyword={quote(search_keyword)}"
        r_s = requests.get(search_url, timeout=10)
        codes = re.findall(r'japic_code=(\d+)', r_s.text + r_s.url)
        if codes:
            jid = codes[0].zfill(8)
            ri = requests.get(f"https://www.kegg.jp/medicus-bin/japic_med?id={jid}")
            ri.encoding = ri.apparent_encoding
            soup = BeautifulSoup(ri.text, 'html.parser')
            th = soup.find('th', string=re.compile(r'æ¬§æ–‡ä¸€èˆ¬å'))
            if th and th.find_next_sibling('td'):
                kegg_en = th.find_next_sibling('td').get_text(strip=True)
                return kegg_en, "KEGG"
    except:
        pass

    return "[ç¿»è­¯å¤±æ•—]", "None"

def parse_full_506(file):
    all_data = []
    current_cat = "æœªçŸ¥"
    
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            # æ›´æ–°é¡åˆ¥åˆ¤å®š
            if "(1)" in text: current_cat = "Cat A"
            elif "(2)" in text: current_cat = "Cat B"
            elif "(3)" in text: current_cat = "Cat C"

            # ç­–ç•¥ A: æŠ“å–æ¨™æº–è¡¨æ ¼ (å‰10é )
            tables = page.extract_tables()
            for table in tables:
                for row in table:
                    if len(row) >= 3:
                        route_raw = str(row[0])
                        # åªè¦åŒ…å«é—œéµå­—å°±æŠ“å–
                        if any(r in route_raw for r in ['å†…', 'æ³¨', 'å¤–']):
                            clean_route = "".join(set(re.findall(r'å†…|æ³¨|å¤–', route_raw)))
                            all_data.append({
                                "é¡åˆ¥": current_cat,
                                "çµ¦è—¥æ–¹å¼": clean_route,
                                "ç”¨é€”é¡åˆ¥": str(row[1]).strip().split('\n')[0],
                                "æˆåˆ†æ—¥æ–‡å": str(row[2]).strip().replace('\n', '')
                            })

            # ç­–ç•¥ B: é‡å°ç¬¬11é å¾Œçš„ã€Œç´”æ–‡å­—è¡Œã€é€²è¡Œ Regex è£œæŠ“
            lines = text.split('\n')
            for line in lines:
                # åŒ¹é…æ ¼å¼ï¼šçµ¦è—¥æ–¹å¼(å†…/æ³¨/å¤–) + 3ä½æ•¸å­— + æˆåˆ†å
                match = re.search(r'^(å†…|æ³¨|å¤–)\s+(\d{3})\s+(.+)$', line.strip())
                if match:
                    route, code, name = match.groups()
                    # æª¢æŸ¥é‡è¤‡ï¼Œé¿å…èˆ‡ç­–ç•¥ A æŠ“åˆ°çš„é‡ç–Š
                    if not any(d['æˆåˆ†æ—¥æ–‡å'] == name for d in all_data):
                        all_data.append({
                            "é¡åˆ¥": current_cat,
                            "çµ¦è—¥æ–¹å¼": route,
                            "ç”¨é€”é¡åˆ¥": code,
                            "æˆåˆ†æ—¥æ–‡å": name
                        })
    return pd.DataFrame(all_data)
# --- 3. Streamlit UI ---
st.title("ğŸ’Š 506é …è—¥å“å…¨è§£æ (Azure å„ªå…ˆæ¨¡å¼)")

f = st.file_uploader("ä¸Šå‚³ PDF", type=['pdf'])

if f:
    if 'raw_df' not in st.session_state:
        st.session_state.raw_df = parse_medicine_pdf(f)
    
    df = st.session_state.raw_df
    st.write(f"å·²å¾ PDF æå– {len(df)} é …æˆåˆ†æ¸…å–®ã€‚")
    st.dataframe(df.head(10)) # å…ˆé è¦½å‰10é …ç¢ºä¿æ—¥æ–‡åæ²’æŠ“éŒ¯

    if st.button("ğŸš€ é–‹å§‹å…¨é‡ç¿»è­¯ (å…ˆ Azure å¾Œ KEGG)"):
        final_results = []
        bar = st.progress(0)
        status = st.empty()
        
        for i, row in df.iterrows():
            jp_name = row["æˆåˆ†æ—¥æ–‡å"]
            status.text(f"è™•ç†ä¸­ ({i+1}/{len(df)}): {jp_name}")
            
            # åŸ·è¡Œé›™é‡å°ç…§é‚è¼¯
            en_name, source = get_english_name(jp_name)
            
            final_results.append({
                "é¡åˆ¥": row["é¡åˆ¥"],
                "çµ¦è—¥æ–¹å¼": row["çµ¦è—¥æ–¹å¼"],
                "ç”¨é€”é¡åˆ¥": row["ç”¨é€”é¡åˆ¥"],
                "æˆåˆ†æ—¥æ–‡å": jp_name,
                "æˆåˆ†è‹±æ–‡å": en_name,
                "å°ç…§ä¾†æº": source
            })
            bar.progress((i + 1) / len(df))
            
        res_df = pd.DataFrame(final_results)
        st.success("å…¨éƒ¨è§£æå®Œæˆï¼")
        st.dataframe(res_df)
        
        # åŒ¯å‡º CSV (UTF-8-SIG ç¢ºä¿ Excel ä¸äº‚ç¢¼)
        csv_data = res_df.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´ CSV å ±å‘Š", csv_data, "Japan_Medicine_Full_Report.csv")
