import streamlit as st
import pandas as pd
import requests
import io
import re
import urllib.parse

# --- 1. åŸºç¤å·¥å…·å‡½æ•¸ ---
def normalize_for_match(text):
    if not isinstance(text, str): return ""
    # è½‰åŠå½¢
    text = text.translate(str.maketrans(
        'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï¼ˆï¼‰',
        '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ()'
    ))
    # ç§»é™¤è—¥å…¸èˆ‡å‚™è¨»ç¬¦è™Ÿ (æ¯”å°ç”¨)
    text = re.sub(r'\(JP\d+.*?\)', '', text)
    text = re.sub(r'\(USP.*?\)', '', text)
    text = re.sub(r'\(NF.*?\)', '', text)
    text = re.sub(r'[â€»\*]\d+', '', text)
    text = text.replace('ï¼', '-').replace(' ', '').replace('ã€€', '').replace('\n', '')
    return text.strip()

# --- 2. å¤–éƒ¨ç¿»è­¯è³‡æº ---
def translate_via_wiki(jap_text):
    """é€é Wiki ç²å–å°æ‡‰è‹±æ–‡æ¨™é¡Œ"""
    try:
        url = f"https://ja.wikipedia.org/w/api.php?action=query&prop=langlinks&lllang=en&titles={urllib.parse.quote(jap_text)}&format=json"
        res = requests.get(url, timeout=5).json()
        pages = res.get('query', {}).get('pages', {})
        for k, v in pages.items():
            if 'langlinks' in v:
                return v['langlinks'][0]['*']
    except:
        pass
    return None

def translate_via_azure(text, api_key, region):
    """Azure ç¿»è­¯"""
    if not api_key or not region or not text: return None
    endpoint = "https://api.cognitive.microsofttranslator.com/translate"
    params = {'api-version': '3.0', 'from': 'ja', 'to': 'en'}
    headers = {
        'Ocp-Apim-Subscription-Key': api_key,
        'Ocp-Apim-Subscription-Region': region,
        'Content-type': 'application/json'
    }
    body = [{'text': text}]
    try:
        res = requests.post(endpoint, params=params, headers=headers, json=body, timeout=5)
        return res.json()[0]['translations'][0]['text']
    except:
        return None

# --- 3. æ ¸å¿ƒè™•ç†é‚è¼¯ ---
def process_drug_data(df, azure_key, azure_region):
    # æ¬„ä½å®šç¾©
    COL_JAP = 'æˆåˆ†å (æ—¥)'
    COL_ENG = 'æˆåˆ†å (è‹±)'
    COL_ID = 'KEGG_ID'
    COL_CAT_JAP = 'è—¥æ•ˆåˆ†é¡'
    COL_CAT_ENG = 'è—¥æ•ˆåˆ†é¡ (è‹±)'

    # ç¢ºä¿è‹±æ–‡åˆ†é¡æ¬„ä½å­˜åœ¨
    if COL_CAT_ENG not in df.columns:
        df[COL_CAT_ENG] = ""

    # ä¸‹è¼‰ KEGG
    try:
        kegg_res = requests.get("https://rest.kegg.jp/list/dr_ja", timeout=20)
        kegg_ref = []
        for line in kegg_res.text.strip().split('\n'):
            parts = line.split('\t')
            if len(parts) < 2: continue
            d_id = "dr_ja:" + parts[0].replace("dr:", "")
            full_info = parts[1]
            eng_match = re.search(r'\(([^)]+)\)$', full_info)
            kegg_ref.append({
                'id': d_id,
                'match_name': normalize_for_match(full_info),
                'eng': eng_match.group(1) if eng_match else ""
            })
    except:
        st.error("KEGG è³‡æ–™åº«é€£ç·šå¤±æ•—")
        return None

    progress_bar = st.progress(0)
    total = len(df)

    for i, row in df.iterrows():
        # --- A. æˆåˆ†åèˆ‡ ID è™•ç† (KEGG) ---
        jap_raw = str(row[COL_JAP])
        jap_clean = normalize_for_match(jap_raw)
        
        # è£œé½Š KEGG_ID èˆ‡æˆåˆ†è‹±æ–‡
        if pd.isna(row.get(COL_ID)) or str(row.get(COL_ID)).strip() in ["", "nan"]:
            for ref in kegg_ref:
                if jap_clean in ref['match_name'] or \
                   ('ãƒ»' in jap_clean and all(p in ref['match_name'] for p in jap_clean.split('ãƒ»'))):
                    df.at[i, COL_ID] = ref['id']
                    if pd.isna(row.get(COL_ENG)) or str(row.get(COL_ENG)).strip() == "":
                        df.at[i, COL_ENG] = ref['eng']
                    break

        # --- B. æˆåˆ†åè£œé½Š (Wiki/Azure) ---
        if pd.isna(df.at[i, COL_ENG]) or str(df.at[i, COL_ENG]).strip() in ["", "nan"]:
            wiki_res = translate_via_wiki(jap_clean)
            if wiki_res:
                df.at[i, COL_ENG] = f"{wiki_res} (Wiki)"
            else:
                azure_res = translate_via_azure(jap_raw, azure_key, azure_region)
                if azure_res:
                    df.at[i, COL_ENG] = f"{azure_res} (Azure)"

        # --- C. è—¥æ•ˆåˆ†é¡ç¿»è­¯ (Wiki/Azure) ---
        cat_jap = str(row.get(COL_CAT_JAP, ""))
        if cat_jap and cat_jap != "nan":
            # å„ªå…ˆå˜—è©¦ Wiki
            cat_wiki = translate_via_wiki(cat_jap)
            if cat_wiki:
                df.at[i, COL_CAT_ENG] = cat_wiki
            else:
                # å¤±æ•—å‰‡ç”¨ Azure
                cat_azure = translate_via_azure(cat_jap, azure_key, azure_region)
                if cat_azure:
                    df.at[i, COL_CAT_ENG] = cat_azure

        progress_bar.progress((i + 1) / total)
    
    return df

# --- 4. UI ---
st.set_page_config(page_title="è—¥å“æ¸…å–®ç¿»è­¯è£œé½Šç³»çµ±", layout="wide")
st.title("ğŸ’Š è—¥å“æ¸…å–®å…¨æ–¹ä½è£œé½Šç³»çµ±")
st.markdown("è£œé½Š `KEGG_ID`ã€`æˆåˆ†å (è‹±)` ä¸¦å°‡ `è—¥æ•ˆåˆ†é¡` ç¿»è­¯ç‚ºè‹±æ–‡ã€‚")

with st.sidebar:
    st.header("ğŸ”‘ API è¨­å®š")
    az_key = st.text_input("Azure API Key", type="password")
    az_region = st.text_input("Azure Region", value="eastasia")

uploaded_file = st.file_uploader("ä¸Šå‚³ CSV æª”æ¡ˆ", type=['csv'])

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.write("### åŸå§‹è³‡æ–™é è¦½")
    st.dataframe(df.head(5))

    if st.button("å•Ÿå‹•è‡ªå‹•è£œé½Šèˆ‡ç¿»è­¯"):
        with st.spinner("ç¨‹åºåŸ·è¡Œä¸­ï¼Œè«‹ç¨å€™..."):
            result_df = process_drug_data(df.copy(), az_key, az_region)
            
            if result_df is not None:
                st.success("å…¨éƒ¨å®Œæˆï¼")
                
                # çµ±è¨ˆ
                k_filled = result_df['KEGG_ID'].notna().sum()
                c_filled = (result_df['è—¥æ•ˆåˆ†é¡ (è‹±)'].str.strip() != "").sum()
                
                c1, c2 = st.columns(2)
                c1.metric("KEGG è£œé½Šæ•¸", k_filled)
                c2.metric("åˆ†é¡ç¿»è­¯æ•¸", c_filled)

                st.dataframe(result_df)

                # ä¸‹è¼‰
                output = io.BytesIO()
                result_df.to_csv(output, index=False, encoding='utf-8-sig')
                st.download_button("ğŸ“¥ ä¸‹è¼‰æ›´æ–°å¾Œçš„ CSV", output.getvalue(), "Drug_List_Translated.csv")
