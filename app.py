import streamlit as st
import pandas as pd
import requests
import io
import re
import urllib.parse

# --- 1. åŸºç¤å·¥å…·å‡½æ•¸ ---
def clean_for_match(text):
    if not isinstance(text, str): return ""
    text = text.translate(str.maketrans(
        'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï¼ˆï¼‰',
        '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ()'
    ))
    text = re.sub(r'\(JP\d+.*?\)', '', text)
    text = re.sub(r'\(USP.*?\)', '', text)
    text = re.sub(r'\(NF.*?\)', '', text)
    text = re.sub(r'[â€»\*]\d+', '', text)
    text = text.replace('ï¼', '-').replace(' ', '').replace('ã€€', '')
    return text.strip()

# --- 2. å¤–éƒ¨ç¿»è­¯è³‡æºå‡½æ•¸ ---

# A. Wikipedia ç¿»è­¯ (åˆ©ç”¨ Wiki çš„èªè¨€éˆæ¥)
def translate_via_wiki(jap_name):
    try:
        # å…ˆæ‰¾æ—¥æ–‡ Wiki é é¢
        search_url = f"https://ja.wikipedia.org/w/api.php?action=query&prop=langlinks&lllang=en&titles={urllib.parse.quote(jap_name)}&format=json"
        res = requests.get(search_url, timeout=5).json()
        pages = res.get('query', {}).get('pages', {})
        for k, v in pages.items():
            if 'langlinks' in v:
                return v['langlinks'][0]['*'] # è¿”å›è‹±æ–‡é é¢æ¨™é¡Œ
    except:
        pass
    return None

# B. Azure Translator ç¿»è­¯
def translate_via_azure(text, api_key, region):
    if not api_key or not region: return None
    endpoint = "https://api.cognitive.microsofttranslator.com/translate"
    params = {'api-version': '3.0', 'from': 'ja', 'to': 'en'}
    headers = {
        'Ocp-Apim-Subscription-Key': api_key,
        'Ocp-Apim-Subscription-Region': region,
        'Content-type': 'application/json'
    }
    body = [{'text': text}]
    try:
        res = requests.post(endpoint, params=params, headers=headers, json=body, timeout=5)
        return res.json()[0]['translations'][0]['text']
    except:
        return None

# --- 3. æ ¸å¿ƒè™•ç†é‚è¼¯ ---
def fetch_and_fill_all_sources(input_df, azure_key, azure_region):
    target_col = 'æˆåˆ†å (æ—¥)'
    eng_col = 'æˆåˆ†å (è‹±)'
    id_col = 'KEGG_ID'

    # å–å¾— KEGG å°ç…§è¡¨
    url = "https://rest.kegg.jp/list/dr_ja"
    kegg_res = requests.get(url, timeout=20)
    kegg_ref = []
    for line in kegg_res.text.strip().split('\n'):
        parts = line.split('\t')
        if len(parts) < 2: continue
        d_id = "dr_ja:" + parts[0].replace("dr:", "")
        full_info = parts[1]
        eng_match = re.search(r'\(([^)]+)\)$', full_info)
        kegg_ref.append({
            'id': d_id,
            'cleaned_name': clean_for_match(full_info),
            'eng': eng_match.group(1) if eng_match else ""
        })

    # åŸ·è¡Œé€è¡Œè£œé½Š
    progress_bar = st.progress(0)
    total = len(input_df)

    for i, row in input_df.iterrows():
        jap_name = str(row[target_col])
        clean_name = clean_for_match(jap_name)
        
        # ç¬¬ä¸€æ­¥ï¼šå˜—è©¦ KEGG (ID + è‹±æ–‡å)
        if pd.isna(row.get(id_col)) or str(row.get(id_col)).strip() in ["", "nan"]:
            found_id, found_eng = None, None
            # æ¨¡ç³Šæ¯”å°é‚è¼¯... (ç°¡åŒ–ç‰ˆ)
            for ref in kegg_ref:
                if clean_name in ref['cleaned_name']:
                    found_id, found_eng = ref['id'], ref['eng']
                    break
            
            if found_id:
                input_df.at[i, id_col] = found_id
                if pd.isna(row.get(eng_col)) or str(row.get(eng_col)).strip() == "":
                    input_df.at[i, eng_col] = found_eng

        # ç¬¬äºŒæ­¥ï¼šå¦‚æœè‹±æ–‡åä»ç‚ºç©ºï¼Œå˜—è©¦ Wikipedia
        if pd.isna(input_df.at[i, eng_col]) or str(input_df.at[i, eng_col]).strip() == "":
            wiki_eng = translate_via_wiki(clean_name)
            if wiki_eng:
                input_df.at[i, eng_col] = f"{wiki_eng} (Wiki)"

        # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœè‹±æ–‡åä»ç‚ºç©ºï¼Œå˜—è©¦ Azure Translator
        if pd.isna(input_df.at[i, eng_col]) or str(input_df.at[i, eng_col]).strip() == "":
            azure_eng = translate_via_azure(jap_name, azure_key, azure_region)
            if azure_eng:
                input_df.at[i, eng_col] = f"{azure_eng} (Azure)"

        progress_bar.progress((i + 1) / total)
    
    return input_df

# --- 4. Streamlit UI ---
st.title("ğŸ’Š è—¥å“å…¨æ–¹ä½ç¿»è­¯èˆ‡è£œé½Šç³»çµ±")

with st.sidebar:
    st.header("API è¨­å®š")
    azure_key = st.text_input("Azure API Key", type="password")
    azure_region = st.text_input("Azure Region (å¦‚ eastasia)")

uploaded_file = st.file_uploader("ä¸Šå‚³ CSV æª”æ¡ˆ", type=['csv'])

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    if st.button("åŸ·è¡Œå¤šå±¤ç´šè£œé½Š"):
        result_df = fetch_and_fill_all_sources(df.copy(), azure_key, azure_region)
        st.success("è£œé½Šå®Œæˆï¼")
        
        # çµ±è¨ˆä¾†æº
        azure_count = result_df[eng_col].str.contains("(Azure)", na=False).sum()
        wiki_count = result_df[eng_col].str.contains("(Wiki)", na=False).sum()
        kegg_count = result_df[id_col].notna().sum()
        
        st.write(f"ğŸ“Š çµ±è¨ˆï¼šKEGG è£œé½Š {kegg_count} é … | Wiki ç¿»è­¯ {wiki_count} é … | Azure ç¿»è­¯ {azure_count} é …")
        st.dataframe(result_df)
        
        csv = result_df.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è¼‰çµæœ", data=csv, file_name="MultiSource_Drug_List.csv")
