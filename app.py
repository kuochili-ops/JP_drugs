import streamlit as st
import pandas as pd
import requests
import io
import re
import urllib.parse

# --- 1. åŸºç¤å·¥å…·å‡½æ•¸ ---
def normalize_for_match(text):
    """åƒ…ä¾›æ¯”å°ä½¿ç”¨çš„æ¸…æ´—é‚è¼¯ï¼šè½‰åŠå½¢ã€ç§»é™¤å‚™è¨»èˆ‡è—¥å…¸æ¨™è¨˜"""
    if not isinstance(text, str): return ""
    # è½‰åŠå½¢
    text = text.translate(str.maketrans(
        'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï¼ˆï¼‰',
        '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ()'
    ))
    # æ¯”å°æ™‚å¿½ç•¥è—¥å…¸æ¨™è¨˜ (JP/USP/NF)
    text = re.sub(r'\(JP\d+.*?\)', '', text)
    text = re.sub(r'\(USP.*?\)', '', text)
    text = re.sub(r'\(NF.*?\)', '', text)
    # å¿½ç•¥ â€» æˆ– * å‚™è¨»ç¬¦è™Ÿ
    text = re.sub(r'[â€»\*]\d+', '', text)
    # è™•ç† L/D å‰ç¶´ç¬¦è™Ÿèˆ‡ç©ºç™½
    text = text.replace('ï¼', '-').replace(' ', '').replace('ã€€', '').replace('\n', '')
    return text.strip()

# --- 2. å¤–éƒ¨ç¿»è­¯è³‡æº ---
def translate_via_wiki(jap_name):
    """é€é Wikipedia èªè¨€éˆçµç²å–å­¸å"""
    try:
        search_url = f"https://ja.wikipedia.org/w/api.php?action=query&prop=langlinks&lllang=en&titles={urllib.parse.quote(jap_name)}&format=json"
        res = requests.get(search_url, timeout=5).json()
        pages = res.get('query', {}).get('pages', {})
        for k, v in pages.items():
            if 'langlinks' in v:
                return v['langlinks'][0]['*']
    except:
        pass
    return None

def translate_via_azure(text, api_key, region):
    """é€é Azure Translator ç¿»è­¯"""
    if not api_key or not region: return None
    endpoint = "https://api.cognitive.microsofttranslator.com/translate"
    params = {'api-version': '3.0', 'from': 'ja', 'to': 'en'}
    headers = {
        'Ocp-Apim-Subscription-Key': api_key,
        'Ocp-Apim-Subscription-Region': region,
        'Content-type': 'application/json'
    }
    body = [{'text': text}]
    try:
        res = requests.post(endpoint, params=params, headers=headers, json=body, timeout=5)
        return res.json()[0]['translations'][0]['text']
    except:
        return None

# --- 3. æ ¸å¿ƒè™•ç†å‡½æ•¸ ---
def fetch_and_process_data(input_df, azure_key, azure_region):
    TARGET_COL = 'æˆåˆ†å (æ—¥)'
    ENG_COL = 'æˆåˆ†å (è‹±)'
    ID_COL = 'KEGG_ID'

    # ä¸‹è¼‰ KEGG è³‡æ–™åº«
    try:
        kegg_res = requests.get("https://rest.kegg.jp/list/dr_ja", timeout=20)
        kegg_res.raise_for_status()
    except:
        st.error("ç„¡æ³•é€£ç·šè‡³ KEGG è³‡æ–™åº«")
        return None

    kegg_ref = []
    for line in kegg_res.text.strip().split('\n'):
        parts = line.split('\t')
        if len(parts) < 2: continue
        d_id = "dr_ja:" + parts[0].replace("dr:", "")
        full_info = parts[1]
        eng_match = re.search(r'\(([^)]+)\)$', full_info)
        kegg_ref.append({
            'id': d_id,
            'match_name': normalize_for_match(full_info),
            'eng': eng_match.group(1) if eng_match else ""
        })

    # é€è¡ŒåŸ·è¡Œè£œé½Š
    progress_bar = st.progress(0)
    total = len(input_df)

    for i, row in input_df.iterrows():
        jap_raw = str(row[TARGET_COL])
        jap_clean = normalize_for_match(jap_raw)
        
        # A. ç¬¬ä¸€å„ªå…ˆï¼šKEGG è£œé½Š
        if pd.isna(row.get(ID_COL)) or str(row.get(ID_COL)).strip() in ["", "nan"]:
            for ref in kegg_ref:
                # æ¨¡ç³Šæ¯”å°ï¼šåŒ…å«æˆ–æ‹†è§£æ¯”å°
                if jap_clean in ref['match_name'] or \
                   ('ãƒ»' in jap_clean and all(p in ref['match_name'] for p in jap_clean.split('ãƒ»'))):
                    input_df.at[i, ID_COL] = ref['id']
                    if pd.isna(row.get(ENG_COL)) or str(row.get(ENG_COL)).strip() == "":
                        input_df.at[i, ENG_COL] = ref['eng']
                    break

        # B. ç¬¬äºŒå„ªå…ˆï¼šWikipedia ç¿»è­¯ (è‹¥è‹±æ–‡åä»ç‚ºç©º)
        current_eng = str(input_df.at[i, ENG_COL])
        if pd.isna(input_df.at[i, ENG_COL]) or current_eng.strip() in ["", "nan"]:
            wiki_res = translate_via_wiki(jap_clean)
            if wiki_res:
                input_df.at[i, ENG_COL] = f"{wiki_res} (Wiki)"

        # C. ç¬¬ä¸‰å„ªå…ˆï¼šAzure ç¿»è­¯ (è‹¥è‹±æ–‡åä»ç‚ºç©º)
        current_eng = str(input_df.at[i, ENG_COL])
        if pd.isna(input_df.at[i, ENG_COL]) or current_eng.strip() in ["", "nan"]:
            azure_res = translate_via_azure(jap_raw, azure_key, azure_region)
            if azure_res:
                input_df.at[i, ENG_COL] = f"{azure_res} (Azure)"

        progress_bar.progress((i + 1) / total)
    
    return input_df

# --- 4. Streamlit UI ---
st.set_page_config(page_title="è—¥å“è³‡æ–™è‡ªå‹•è£œé½Šç³»çµ±", layout="wide")
st.title("ğŸ’Š è—¥å“è³‡æ–™æ™ºæ…§è£œé½Šç³»çµ± (æ•´åˆç‰ˆ)")

with st.sidebar:
    st.header("ğŸ”‘ ç¿»è­¯ API è¨­å®š")
    az_key = st.text_input("Azure API Key", type="password")
    az_region = st.text_input("Azure Region (å¦‚ eastasia)")
    st.info("å¦‚ç„¡ Azure é‡‘é‘°ï¼Œç³»çµ±å°‡åƒ…ä½¿ç”¨ KEGG èˆ‡ Wikipedia è³‡æºã€‚")

uploaded_file = st.file_uploader("ä¸Šå‚³ CSV æª”æ¡ˆ (æ¬„ä½éœ€åŒ…å« 'æˆåˆ†å (æ—¥)')", type=['csv'])

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.write("### åŸå§‹è³‡æ–™é è¦½")
    st.dataframe(df.head(5))

    if st.button("å•Ÿå‹•å¤šå±¤ç´šè£œé½Šç¨‹åº"):
        with st.spinner("æ­£åœ¨åŸ·è¡Œè·¨è³‡æ–™åº«æª¢ç´¢èˆ‡ç¿»è­¯..."):
            result_df = fetch_and_process_data(df.copy(), az_key, az_region)
            
            if result_df is not None:
                st.success("ç¨‹åºåŸ·è¡Œå®Œç•¢ï¼")
                
                # çµ±è¨ˆæ•¸æ“š (ä¿®æ­£ NameError å•é¡Œï¼Œç›´æ¥ä½¿ç”¨å­—ä¸²)
                k_filled = result_df['KEGG_ID'].notna().sum()
                w_filled = result_df['æˆåˆ†å (è‹±)'].str.contains(r'\(Wiki\)', na=False).sum()
                a_filled = result_df['æˆåˆ†å (è‹±)'].str.contains(r'\(Azure\)', na=False).sum()
                final_miss = result_df['æˆåˆ†å (è‹±)'].isna().sum()

                c1, c2, c3, c4 = st.columns(4)
                c1.metric("KEGG æˆåŠŸæ•¸", f"{k_filled}")
                c2.metric("Wiki ç¿»è­¯æ•¸", f"{w_filled}")
                c3.metric("Azure ç¿»è­¯æ•¸", f"{a_filled}")
                c4.metric("å‰©é¤˜ç©ºç¼º", f"{final_miss}")

                if final_miss > 0:
                    with st.expander("ğŸ” æª¢è¦–ç„¡æ³•è‡ªå‹•è£œé½Šçš„é …ç›®"):
                        st.table(result_df[result_df['æˆåˆ†å (è‹±)'].isna()][['æˆåˆ†å (æ—¥)']])

                st.subheader("è™•ç†çµæœ")
                st.dataframe(result_df)

                # ä¸‹è¼‰
                output = io.BytesIO()
                result_df.to_csv(output, index=False, encoding='utf-8-sig')
                st.download_button("ğŸ“¥ ä¸‹è¼‰ä¿®æ­£å¾Œçš„ CSV", data=output.getvalue(), file_name="Drug_List_Full_Updated.csv")
