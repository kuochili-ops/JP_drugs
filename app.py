import streamlit as st
import pandas as pd
import requests
import re
import time
from urllib.parse import quote
from bs4 import BeautifulSoup

# --- 1. é…ç½®å€åŸŸ ---
AZURE_KEY = "9JDF24qrsW8rXiYmChS17yEPyNRI96nNXXqEKn5CyI6ql6iYcTOFJQQJ99BLAC3pKaRXJ3w3AAAbACOGVYVU"
AZURE_ENDPOINT = "https://api.cognitive.microsofttranslator.com"
AZURE_REGION = "eastasia"

def get_english_test(jp_name):
    """ æ¸¬è©¦å°ˆç”¨ï¼šWikipedia -> KEGG -> Azure ä¸‰å±¤æª¢ç´¢ """
    if not jp_name or pd.isna(jp_name): return "N/A", "Skip"
    
    # å¼·åŠ›æ¸…æ´—æ—¥æ–‡ (ç§»é™¤æ‹¬è™Ÿã€æ°´å’Œç‰©ã€é¹½é¡)
    clean_ja = re.sub(r'[\(\ï¼ˆ].*?[\)\ï¼‰]', '', str(jp_name)).strip()
    clean_ja = re.sub(r'(æ°´å’Œç‰©|å¡©é…¸å¡©|ã‚«ãƒªã‚¦ãƒ |ãƒŠãƒˆãƒªã‚¦ãƒ |è‡­åŒ–ç‰©|ã‚¨ã‚¹ãƒ†ãƒ«)$', '', clean_ja)

    # 1. Wikipedia API (æœ€å¿«ä¸”å°ç‰‡å‡åæœ€æº–)
    try:
        w_url = f"https://ja.wikipedia.org/w/api.php?action=query&titles={quote(clean_ja)}&prop=langlinks&lllang=en&format=json"
        w_res = requests.get(w_url, timeout=5).json()
        pages = w_res.get('query', {}).get('pages', {})
        for p in pages.values():
            if 'langlinks' in p:
                return p['langlinks'][0]['*'], "Wikipedia"
    except: pass

    # 2. KEGG Medicus (å®˜æ–¹è—¥å…¸)
    try:
        search_url = f"https://www.kegg.jp/medicus-bin/search_drug?search_keyword={quote(clean_ja)}"
        r = requests.get(search_url, timeout=5)
        codes = re.findall(r'japic_code=(\d+)', r.text + r.url)
        if codes:
            ri = requests.get(f"https://www.kegg.jp/medicus-bin/japic_med?id={codes[0].zfill(8)}")
            ri.encoding = 'shift_jis'
            soup = BeautifulSoup(ri.text, 'html.parser')
            th = soup.find('th', string=re.compile(r'æ¬§æ–‡ä¸€èˆ¬å'))
            if th and th.find_next_sibling('td'):
                return th.find_next_sibling('td').get_text(strip=True), "KEGG"
    except: pass

    # 3. Azure (æœ€å¾Œæ‰‹æ®µ)
    try:
        url = f"{AZURE_ENDPOINT.strip('/')}/translate?api-version=3.0&from=ja&to=en"
        headers = {'Ocp-Apim-Subscription-Key': AZURE_KEY, 'Ocp-Apim-Subscription-Region': AZURE_REGION, 'Content-type': 'application/json'}
        res = requests.post(url, headers=headers, json=[{'text': clean_ja}], timeout=5)
        if res.status_code == 200:
            return res.json()[0]['translations'][0]['text'], "Azure"
    except: pass

    return "[å°ç…§å¤±æ•—]", "None"

# --- 2. UI æ¸¬è©¦ä»‹é¢ ---
st.title("ğŸ§ª å‰ 10 é …å°ç…§å£“åŠ›æ¸¬è©¦")

f = st.file_uploader("ä¸Šå‚³ 2026-01-08T06-33_export.csv", type=['csv'])

if f:
    df_all = pd.read_csv(f)
    test_df = df_all.head(10).copy() # åªå–å‰ 10 é …
    
    st.write("ğŸ” **é è¨ˆæ¸¬è©¦çš„å‰ 10 é …æ—¥æ–‡æˆåˆ†ï¼š**")
    st.write(", ".join(test_df["æˆåˆ†æ—¥æ–‡å"].tolist()))

    if st.button("ğŸš€ é–‹å§‹æ¸¬è©¦å‰ 10 é …"):
        results = []
        bar = st.progress(0)
        for i, row in test_df.iterrows():
            jp = row["æˆåˆ†æ—¥æ–‡å"]
            en, src = get_english_test(jp)
            results.append({"æˆåˆ†æ—¥æ–‡å": jp, "æˆåˆ†è‹±æ–‡å": en, "å°ç…§ä¾†æº": src})
            bar.progress((i + 1) / 10)
        
        res_df = pd.DataFrame(results)
        st.success("âœ… æ¸¬è©¦å®Œæˆï¼è«‹æª¢æŸ¥ä¸‹æ–¹å°ç…§çµæœï¼š")
        st.table(res_df) # ä½¿ç”¨è¡¨æ ¼é¡¯ç¤ºæ›´æ¸…æ™°

        # åˆ¤æ–·æˆåŠŸç‡
        success_count = len(res_df[res_df["å°ç…§ä¾†æº"] != "None"])
        if success_count >= 8:
            st.balloons()
            st.info(f"æˆåŠŸç‡ {success_count}/10ï¼Œå»ºè­°å¯ä»¥åŸ·è¡Œå…¨é‡å°ç…§ã€‚")
        else:
            st.warning(f"æˆåŠŸç‡åƒ… {success_count}/10ï¼Œå¯èƒ½éœ€è¦èª¿æ•´æ¸…æ´—é‚è¼¯ã€‚")
