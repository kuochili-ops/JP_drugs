import streamlit as st
import pandas as pd
import requests
import re
import time
import io
from urllib.parse import quote
from bs4 import BeautifulSoup

# --- 1. é…ç½®å€åŸŸ ---
AZURE_KEY = "9JDF24qrsW8rXiYmChS17yEPyNRI96nNXXqEKn5CyI6ql6iYcTOFJQQJ99BLAC3pKaRXJ3w3AAAbACOGVYVU"
AZURE_ENDPOINT = "https://api.cognitive.microsofttranslator.com"
AZURE_REGION = "eastasia"

# --- 2. æ ¸å¿ƒå¢å¼·ç¿»è­¯é‚è¼¯ ---

def clean_japanese_name(name):
    """ å¼·åŠ›æ¸…æ´—æ—¥æ–‡åï¼Œåªä¿ç•™æ ¸å¿ƒæˆåˆ† """
    if not name or pd.isna(name): return ""
    # 1. ç§»é™¤æ‹¬è™ŸåŠå…¶å…§å®¹ (å¦‚ï¼š(æ°´å’Œç‰©)ã€ï¼ˆãƒ™ãƒãƒˆãƒªãƒ³ï¼‰)
    name = re.sub(r'[\(\ï¼ˆ].*?[\)\ï¼‰]', '', str(name))
    # 2. ç§»é™¤å¸¸è¦‹å¾Œç¶´ä»¥æé«˜ API å‘½ä¸­ç‡
    suffixes = ['æ°´å’Œç‰©', 'å¡©é…¸å¡©', 'ã‚«ãƒªã‚¦ãƒ ', 'ãƒŠãƒˆãƒªã‚¦ãƒ ', 'ã‚¨ã‚¹ãƒ†ãƒ«', 'è‡­åŒ–ç‰©']
    for s in suffixes:
        name = name.replace(s, '')
    return name.strip()

def get_translation_v2(jp_name):
    """ 
    å¤šé‡è£œå®Œç­–ç•¥ï¼š
    1. Azure ç¿»è­¯ (æœ€å¿«)
    2. Wikipedia æ—¥è‹±å°ç…§ (æ¥µæº–)
    3. KEGG å®˜æ–¹è³‡æ–™åº« (æœ€æ¬Šå¨)
    """
    if not jp_name or pd.isna(jp_name): return "N/A", "Skip"
    
    clean_ja = clean_japanese_name(jp_name)
    if not clean_ja: return "N/A", "Skip"

    # --- Step 1: Wikipedia API (æ—¥èªè½‰è‹±èªï¼Œå°è—¥åæ¥µå…¶æœ‰æ•ˆ) ---
    try:
        wiki_url = f"https://ja.wikipedia.org/w/api.php?action=query&titles={quote(clean_ja)}&prop=langlinks&lllang=en&format=json"
        wiki_res = requests.get(wiki_url, timeout=5).json()
        pages = wiki_res.get('query', {}).get('pages', {})
        for p in pages.values():
            if 'langlinks' in p:
                return p['langlinks'][0]['*'], "Wikipedia"
    except: pass

    # --- Step 2: Azure Translator ---
    try:
        url = f"{AZURE_ENDPOINT.strip('/')}/translate?api-version=3.0&from=ja&to=en"
        headers = {'Ocp-Apim-Subscription-Key': AZURE_KEY, 'Ocp-Apim-Subscription-Region': AZURE_REGION, 'Content-type': 'application/json'}
        res = requests.post(url, headers=headers, json=[{'text': clean_ja}], timeout=5)
        if res.status_code == 200:
            en = res.json()[0]['translations'][0]['text']
            if len(en) > 2: return en, "Azure"
    except: pass

    # --- Step 3: KEGG Fallback ---
    try:
        search_url = f"https://www.kegg.jp/medicus-bin/search_drug?search_keyword={quote(clean_ja)}"
        r = requests.get(search_url, timeout=10)
        codes = re.findall(r'japic_code=(\d+)', r.text + r.url)
        if codes:
            ri = requests.get(f"https://www.kegg.jp/medicus-bin/japic_med?id={codes[0].zfill(8)}")
            ri.encoding = 'shift_jis' # KEGG å¸¸ä½¿ç”¨ SJIS æˆ– EUC-JP
            soup = BeautifulSoup(ri.text, 'html.parser')
            th = soup.find('th', string=re.compile(r'æ¬§æ–‡ä¸€èˆ¬å'))
            if th and th.find_next_sibling('td'):
                return th.find_next_sibling('td').get_text(strip=True), "KEGG"
    except: pass

    return "[ä»æœªæ‰¾åˆ°]", "None"

# --- 3. Streamlit UI ---
st.set_page_config(layout="wide", page_title="505é …è£œå®Œ V2")
st.title("ğŸ’Š é†«è—¥å“æ¸…å–®è£œå®Œ (Wikipedia + Azure + KEGG)")

# ä¸Šå‚³æ‚¨å‰›ä¸‹è¼‰çš„é‚£å€‹ CSV
f = st.file_uploader("è«‹ä¸Šå‚³å‰›æ‰å¤±æ•—çš„ 2026-01-08T06-16_export.csv", type=['csv'])

if f:
    df = pd.read_csv(f)
    # ç§»é™¤èˆŠçš„ç´¢å¼•åˆ— (å¦‚æœæœ‰)
    if 'Unnamed: 0' in df.columns: df = df.drop(columns=['Unnamed: 0'])
    
    st.write(f"ğŸ“Š ç›®å‰æ¸…å–®é …ç›®ï¼š{len(df)}")
    st.dataframe(df.head(10), use_container_width=True)

    if st.button("ğŸš€ åŸ·è¡Œçµ‚æ¥µè£œå…¨è¨ˆç•«"):
        bar = st.progress(0)
        status = st.empty()
        
        for i, row in df.iterrows():
            # åªè™•ç†ã€Œä»æœªæ‰¾åˆ°ã€æˆ–è³‡æ–™ä¾†æºç‚º None çš„é …
            curr_en = str(row["æˆåˆ†è‹±æ–‡å"])
            if curr_en in ["[ä»æœªæ‰¾åˆ°]", "None", "nan", ""]:
                jp_name = row["æˆåˆ†æ—¥æ–‡å"]
                status.text(f"æ­£åœ¨æ·±åº¦æª¢ç´¢ ({i+1}/{len(df)}): {jp_name}")
                
                en_name, source = get_translation_v2(jp_name)
                df.at[i, "æˆåˆ†è‹±æ–‡å"] = en_name
                df.at[i, "ä¾†æº"] = source
            
            bar.progress((i + 1) / len(df))
            if i % 10 == 0: time.sleep(0.05)
            
        st.success("ğŸ‰ å…¨é‡ä¿®è£œå®Œæˆï¼")
        st.dataframe(df, use_container_width=True)
        
        # åŒ¯å‡º CSV (UTF-8-SIG ç¢ºä¿ Excel ä¸äº‚ç¢¼)
        csv_out = df.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è¼‰çµ‚æ¥µä¿®æ­£å ±å‘Š", csv_out, "Medicine_Final_Fixed.csv")
