import streamlit as st
import pandas as pd
import requests
import io
import re

# 1. åŸºç¤å·¥å…·å‡½æ•¸ï¼šè™•ç†å…¨å½¢è½‰åŠå½¢ï¼Œä¸¦çµ±ä¸€æ¨™é»ç¬¦è™Ÿ
def normalize_text(text):
    if not isinstance(text, str): return str(text)
    # è½‰æ›å…¨å½¢æ•¸å­—ã€è‹±æ–‡å­—æ¯èˆ‡æ‹¬è™Ÿ
    text = text.translate(str.maketrans(
        'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï¼ˆï¼‰',
        '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ()'
    ))
    # çµ±ä¸€å°‡å¸¸è¦‹çš„åˆ†éš”ç¬¦è™Ÿè½‰ç‚ºæ¨™æº–ä¸­é–“é» 'ãƒ»'
    text = text.replace(' ', '').replace('ã€€', '').replace('/', 'ãƒ»').replace(',', 'ãƒ»')
    return text.strip()

# 2. æ ¸å¿ƒæ¯”å°é‚è¼¯
def smart_match(search_name, kegg_ref):
    """
    search_name: ä½¿ç”¨è€…ä¸Šå‚³çš„æˆåˆ†å
    kegg_ref: KEGG è³‡æ–™åº«çš„åƒè€ƒæ¸…å–®
    """
    normalized_search = normalize_text(search_name)
    
    # ç­–ç•¥ A: å®Œå…¨æˆ–åŒ…å«æ¯”å° (ä¾‹å¦‚ "A" åŒ…å«åœ¨ "A (JAN)")
    for ref in kegg_ref:
        if normalized_search in ref['clean_full']:
            return ref['id'], ref['eng']

    # ç­–ç•¥ B: è¤‡æ–¹æ‹†è§£æ¯”å° (è™•ç† "Aãƒ»B" é€™ç¨®æƒ…æ³)
    if 'ãƒ»' in normalized_search:
        parts = [p for p in normalized_search.split('ãƒ»') if p] # æ‹†åˆ†æˆåˆ†
        for ref in kegg_ref:
            # å¿…é ˆæ‰€æœ‰æ‹†åˆ†çš„æˆåˆ†éƒ½å‡ºç¾åœ¨ KEGG çš„åç¨±ä¸­ (ä¸é™é †åº)
            if all(part in ref['clean_full'] for part in parts):
                return ref['id'], ref['eng']
    
    return None, None

def fetch_and_fill_kegg_data_advanced(input_df):
    target_col = 'æˆåˆ†å (æ—¥)'
    eng_col = 'æˆåˆ†å (è‹±)'
    id_col = 'KEGG_ID'

    # ä¸‹è¼‰ KEGG å°ç…§è¡¨
    url = "https://rest.kegg.jp/list/dr_ja"
    try:
        response = requests.get(url, timeout=20)
        response.raise_for_status()
    except:
        st.error("ç„¡æ³•é€£ç·šè‡³ KEGG è³‡æ–™åº«ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
        return None

    # é è™•ç† KEGG è³‡æ–™ä»¥åŠ å¿«é€Ÿåº¦
    kegg_ref = []
    for line in response.text.strip().split('\n'):
        parts = line.split('\t')
        if len(parts) < 2: continue
        d_id = parts[0].replace("dr:", "")
        full_info = parts[1]
        
        # æå–æ‹¬è™Ÿå…§çš„è‹±æ–‡å (JAN/USP)
        eng_match = re.search(r'\(([^)]+)\)$', full_info)
        eng_name = eng_match.group(1) if eng_match else ""
        
        kegg_ref.append({
            'id': "dr_ja:" + d_id,
            'clean_full': normalize_text(full_info),
            'eng': eng_name
        })

    # åŸ·è¡Œè£œé½Š
    progress_bar = st.progress(0)
    total_rows = len(input_df)
    
    for i, row in input_df.iterrows():
        # åªæœ‰ç•¶ KEGG_ID ç‚ºç©ºæ™‚æ‰å¡«è£œ
        current_id = str(row.get(id_col, ""))
        if pd.isna(row.get(id_col)) or current_id.strip() == "" or current_id == "nan":
            found_id, found_eng = smart_match(row[target_col], kegg_ref)
            if found_id:
                input_df.at[i, id_col] = found_id
                # åªæœ‰ç•¶è‹±æ–‡åä¹Ÿç‚ºç©ºæ™‚æ‰è£œ
                if pd.isna(row.get(eng_col)) or str(row.get(eng_col)).strip() == "":
                    input_df.at[i, eng_col] = found_eng
        
        progress_bar.progress((i + 1) / total_rows)
    
    return input_df

# --- 3. Streamlit UI ---
st.set_page_config(page_title="è—¥å“è³‡æ–™æ™ºæ…§è£œé½Šå™¨", layout="wide")
st.title("ğŸ’Š æ™ºæ…§å‹è—¥å“è³‡æ–™è£œé½Šå·¥å…· (è¤‡æ–¹åŠ å¼·ç‰ˆ)")
st.markdown("""
æœ¬å·¥å…·æœƒè‡ªå‹•è£œé½Š `KEGG_ID` èˆ‡ `æˆåˆ†å (è‹±)`ï¼š
- **æ¨¡ç³Šæ¯”å°**ï¼šè‡ªå‹•è™•ç†å…¨å½¢æ•¸å­— (ï¼”) èˆ‡åŠå½¢ (4) çš„å·®ç•°ã€‚
- **è¤‡æ–¹æ”¯æ´**ï¼šè‡ªå‹•æ‹†è§£ `ãƒ»` éš”é–‹çš„æˆåˆ†ä¸¦é€²è¡Œäº¤å‰æª¢ç´¢ã€‚
""")

uploaded_file = st.file_uploader("ä¸Šå‚³ CSV æª”æ¡ˆ (éœ€åŒ…å« 'æˆåˆ†å (æ—¥)' æ¬„ä½)", type=['csv'])

if uploaded_file:
    # è®€å–è³‡æ–™
    df = pd.read_csv(uploaded_file)
    
    st.write("### åŸå§‹è³‡æ–™é è¦½")
    st.dataframe(df.head(5))

    if st.button("å•Ÿå‹•æ™ºæ…§è£œé½Š"):
        # ç´€éŒ„åŸå§‹ç‹€æ…‹
        initial_missing = df['KEGG_ID'].isna().sum()
        
        with st.spinner("æ­£åœ¨æª¢ç´¢ KEGG ä¸¦åˆ†æè¤‡æ–¹æˆåˆ†..."):
            result_df = fetch_and_fill_kegg_data_advanced(df.copy())
            
            if result_df is not None:
                # è¨ˆç®—çµæœ
                final_missing = result_df['KEGG_ID'].isna().sum()
                filled_count = initial_missing - final_missing
                
                st.success("è£œé½Šç¨‹åºåŸ·è¡Œå®Œç•¢ï¼")
                
                # --- çµ±è¨ˆé¢æ¿ ---
                col1, col2, col3 = st.columns(3)
                col1.metric("æˆåŠŸè£œé½Šæ•¸é‡", f"{filled_count} é …")
                col2.metric("å°šæœªé…å°æ•¸é‡", f"{final_missing} é …", delta=f"-{filled_count}", delta_color="normal")
                col3.metric("è³‡æ–™ç¸½ç­†æ•¸", f"{len(result_df)} ç­†")
                
                # --- é¡¯ç¤ºæœªé…å°æ¸…å–® ---
                if final_missing > 0:
                    with st.expander("ğŸ” æŸ¥çœ‹ç„¡æ³•é…å°çš„é …ç›® (å»ºè­°æ‰‹å‹•æª¢æŸ¥)"):
                        unmatched = result_df[result_df['KEGG_ID'].isna()][['æˆåˆ†å (æ—¥)', 'æˆåˆ†å (è‹±)']]
                        st.table(unmatched)
                
                st.subheader("è£œé½Šå¾Œçš„è³‡æ–™çµæœ")
                st.dataframe(result_df)
                
                # ä¸‹è¼‰æŒ‰éˆ•
                csv_buffer = io.BytesIO()
                result_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰æ›´æ–°å¾Œçš„ CSV",
                    data=csv_buffer.getvalue(),
                    file_name="KEGG_Updated_List.csv",
                    mime="text/csv"
                )
