import streamlit as st
import pdfplumber
import pandas as pd
import requests
import re
import time
import io
from urllib.parse import quote
from bs4 import BeautifulSoup

# --- 1. é…ç½®å€åŸŸ ---
AZURE_KEY = "9JDF24qrsW8rXiYmChS17yEPyNRI96nNXXqEKn5CyI6ql6iYcTOFJQQJ99BLAC3pKaRXJ3w3AAAbACOGVYVU"
AZURE_ENDPOINT = "https://api.cognitive.microsofttranslator.com"
AZURE_REGION = "eastasia"

# æ—¥æœ¬è—¥æ•ˆåˆ†é¡ç•ªè™Ÿå°ç…§è¡¨ (å¸¸ç”¨éƒ¨åˆ†)
PURPOSE_MAP = {
    "111": "å…¨èº«éº»é†‰åŠ‘", "112": "å‚¬çœ é®éœåŠ‘", "113": "æŠ—ç™²ç™‡åŠ‘", "114": "è§£ç†±é®ç—›åŠ‘",
    "116": "æŠ—å·´é‡‘æ£®æ°ç—‡åŠ‘", "117": "ç²¾ç¥ç¥ç¶“ç”¨åŠ‘", "121": "å±€éƒ¨éº»é†‰åŠ‘", "122": "éª¨éª¼è‚Œé¬†å¼›åŠ‘",
    "123": "è‡ªå¾‹ç¥ç¶“åŠ‘", "124": "è§£ç—™åŠ‘", "131": "çœ¼ç§‘ç”¨åŠ‘", "132": "è€³é¼»å–‰ç§‘ç”¨åŠ‘",
    "211": "å¼·å¿ƒåŠ‘", "212": "ä¸æ•´å¾‹ç”¨åŠ‘", "213": "åˆ©å°¿åŠ‘", "214": "è¡€å£“é™ä¸‹åŠ‘",
    "217": "è¡€ç®¡æ“´å¼µåŠ‘", "218": "é«˜è„‚è¡€ç—‡åŠ‘", "219": "å…¶ä»–å¾ªç’°å™¨å®˜ç”¨åŠ‘",
    "221": "å‘¼å¸ä¿ƒé€²åŠ‘", "222": "é®å’³åŠ‘", "223": "ç¥›ç—°åŠ‘", "225": "æ”¯æ°£ç®¡æ“´å¼µåŠ‘",
    "232": "æ¶ˆåŒ–æ€§æ½°ç˜åŠ‘", "233": "å¥èƒƒæ¶ˆåŒ–åŠ‘", "234": "åˆ¶é…¸åŠ‘", "235": "æ­¢ç€‰åŠ‘",
    "239": "å…¶ä»–æ¶ˆåŒ–å™¨å®˜ç”¨åŠ‘", "241": "è…¦ä¸‹å‚é«”æ¿€ç´ ", "243": "ç”²ç‹€è…ºæ¿€ç´ ", "245": "è…ä¸Šè…ºæ¿€ç´ ",
    "247": "åµå·¢æ¿€ç´ ", "249": "å…¶ä»–æ¿€ç´ åŠ‘", "252": "æ³Œå°¿å™¨å®˜ç”¨åŠ‘", "255": "ç—”ç˜¡ç”¨åŠ‘",
    "261": "å¤–ç”¨æ®ºèŒæ¶ˆæ¯’åŠ‘", "264": "é®ç—›æ¶ˆç‚åŠ‘ (å¤–ç”¨)", "311": "ç¶­ç”Ÿç´  D åŠ‘",
    "331": "è¡€æ¶²ä»£ç”¨åŠ‘", "332": "æ­¢è¡€åŠ‘", "333": "è¡€æ¶²å‡å›ºé˜»æ­¢åŠ‘", "339": "å…¶ä»–è¡€æ¶²/é«”æ¶²ç”¨è—¥",
    "391": "è‚è‡Ÿç–¾æ‚£åŠ‘", "392": "è§£æ¯’åŠ‘", "395": "é…µç´ è£½åŠ‘", "396": "ç³–å°¿ç—…ç”¨åŠ‘",
    "399": "å…ç–«æŠ‘åˆ¶åŠ‘/ä»£è¬è—¥", "421": "çƒ·åŒ–åŠ‘", "422": "ä»£è¬æ‹®æŠ—åŠ‘", "423": "æŠ—ç™Œæ€§æŠ—ç”Ÿç´ ",
    "424": "æ¤ç‰©æ€§æŠ—ç™ŒåŠ‘", "429": "å…¶ä»–æŠ—æƒ¡æ€§è…«ç˜¤åŠ‘ (æ¨™é¶è—¥)", "441": "æŠ—çµ„ç¹”èƒºåŠ‘",
    "611": "æŠ—ç”Ÿç´  (é©è˜­æ°é™½æ€§)", "612": "æŠ—ç”Ÿç´  (é©è˜­æ°é™°æ€§)", "613": "å»£æ•ˆæŠ—ç”Ÿç´ ",
    "614": "æŠ—ç”Ÿç´  (å¤§ç’°å…§é…¯)", "615": "æŠ—ç”Ÿç´  (å››ç’°ç´ )", "619": "å…¶ä»–æŠ—ç”Ÿç´ ",
    "624": "åˆæˆæŠ—èŒåŠ‘ (å–¹è«¾é…®)", "625": "æŠ—ç—…æ¯’åŠ‘", "629": "å…¶ä»–åŒ–å­¸ç™‚æ³•åŠ‘",
    "634": "è¡€æ¶²è£½åŠ‘", "639": "ç–«è‹—/ç”Ÿç‰©è£½å“", "711": "è¨ºæ–·ç”¨è—¥", "721": "Xå…‰é€ å½±åŠ‘"
}

# --- 2. æ ¸å¿ƒå°ç…§åŠŸèƒ½ ---

def get_english_name(jp_name):
    """ å…ˆ Azure ç¿»è­¯ -> å¤±æ•—å‰‡ KEGG çˆ¬èŸ² """
    if not jp_name: return "N/A", "Skip"
    clean_ja = re.split(r'[\(\n\sï¼ˆ]', str(jp_name))[0].strip()
    
    # Step 1: Azure
    try:
        url = f"{AZURE_ENDPOINT.strip('/')}/translate?api-version=3.0&from=ja&to=en"
        headers = {'Ocp-Apim-Subscription-Key': AZURE_KEY, 'Ocp-Apim-Subscription-Region': AZURE_REGION, 'Content-type': 'application/json'}
        res = requests.post(url, headers=headers, json=[{'text': clean_ja}], timeout=5)
        if res.status_code == 200:
            en = res.json()[0]['translations'][0]['text']
            if en and len(en) > 2: return en, "Azure"
    except: pass

    # Step 2: KEGG Fallback
    try:
        search_url = f"https://www.kegg.jp/medicus-bin/search_drug?search_keyword={quote(clean_ja)}"
        r_s = requests.get(search_url, timeout=5)
        codes = re.findall(r'japic_code=(\d+)', r_s.text + r_s.url)
        if codes:
            ri = requests.get(f"https://www.kegg.jp/medicus-bin/japic_med?id={codes[0].zfill(8)}")
            ri.encoding = ri.apparent_encoding
            soup = BeautifulSoup(ri.text, 'html.parser')
            th = soup.find('th', string=re.compile(r'æ¬§æ–‡ä¸€èˆ¬å'))
            if th and th.find_next_sibling('td'):
                return th.find_next_sibling('td').get_text(strip=True), "KEGG"
    except: pass
    return "[å°ç…§å¤±æ•—]", "None"

# --- 3. è§£æåŠŸèƒ½ (éŒ¨é»+åˆä½µ) ---

def parse_full_medicine_pdf(file):
    all_data = []
    current_cat = "æœªçŸ¥"
    
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            # é¡åˆ¥åˆ¤å®š
            if "(1)" in text: current_cat = "Cat A (æœ€å„ªå…ˆ)"
            elif "(2)" in text: current_cat = "Cat B (å„ªå…ˆ)"
            elif "(3)" in text: current_cat = "Cat C (ç©©å®šç¢ºä¿)"

            lines = text.split('\n')
            for line in lines:
                line = line.strip()
                if not line or "æˆåˆ†å" in line: continue
                
                # éŒ¨é»åŒ¹é…: (çµ¦è—¥æ–¹å¼) (3ç¢¼) (æˆåˆ†å)
                match = re.search(r'^(å†…|æ³¨|å¤–)\s*(\d{3})\s*(.+)$', line)
                if match:
                    route, code, name = match.groups()
                    all_data.append({
                        "é¡åˆ¥": current_cat,
                        "çµ¦è—¥æ–¹å¼": route,
                        "ç”¨é€”ç·¨è™Ÿ": code,
                        "ç”¨é€”èªªæ˜": PURPOSE_MAP.get(code, "å…¶ä»–è—¥æ•ˆé¡åˆ¥"),
                        "æˆåˆ†æ—¥æ–‡å": name.strip()
                    })
                else:
                    # è·¨è¡Œåˆä½µ
                    if all_data and not re.match(r'^\d+$', line) and "åšç”ŸåŠ´åƒçœ" not in line:
                        all_data[-1]["æˆåˆ†æ—¥æ–‡å"] += line.strip()

    # æ¸…æ´—æ—¥æ–‡åç¨± (ç§»é™¤ç©ºæ ¼èˆ‡å°¾ç«¯é ç¢¼)
    for d in all_data:
        d["æˆåˆ†æ—¥æ–‡å"] = re.sub(r'\s+', '', d["æˆåˆ†æ—¥æ–‡å"])
        d["æˆåˆ†æ—¥æ–‡å"] = re.sub(r'\d+$', '', d["æˆåˆ†æ—¥æ–‡å"])
    
    return pd.DataFrame(all_data)

# --- 4. Streamlit UI ---
st.set_page_config(layout="wide", page_title="505é …é†«è—¥å“å°ç…§")
st.title("ğŸ’Š å®‰å®šç¢ºä¿é†«è—¥å“å…¨é‡å°ç…§ (Azure + KEGG)")
st.write("è§£æè¦å‰‡ï¼šä»¥ã€Œå…§/æ³¨/å¤–ã€å®šæ¨™ï¼Œè‡ªå‹•å°ç…§ä¸‰ç¢¼ç”¨é€”èªªæ˜ã€‚")

f = st.file_uploader("ä¸Šå‚³ PDF (000785498.pdf)", type=['pdf'])

if f:
    if 'raw_df' not in st.session_state:
        with st.spinner("æ­£åœ¨åŸ·è¡Œå®šæ¨™è§£æ..."):
            st.session_state.raw_df = parse_full_medicine_pdf(f)
    
    df = st.session_state.raw_df
    st.success(f"âœ… æˆåŠŸæå– {len(df)} é …æˆåˆ†ï¼")
    st.dataframe(df, use_container_width=True)

    if st.button("ğŸš€ é–‹å§‹å…¨é‡åŸ·è¡Œç¿»è­¯èˆ‡å®˜æ–¹åç¨±å°ç…§"):
        results = []
        bar = st.progress(0)
        status = st.empty()
        
        for i, row in df.iterrows():
            jp_name = row["æˆåˆ†æ—¥æ–‡å"]
            status.text(f"è™•ç†ä¸­ ({i+1}/{len(df)}): {jp_name}")
            
            en_name, source = get_english_name(jp_name)
            
            results.append({
                "é¡åˆ¥": row["é¡åˆ¥"], "çµ¦è—¥æ–¹å¼": row["çµ¦è—¥æ–¹å¼"],
                "ç”¨é€”ç·¨è™Ÿ": row["ç”¨é€”ç·¨è™Ÿ"], "ç”¨é€”èªªæ˜": row["ç”¨é€”èªªæ˜"],
                "æˆåˆ†æ—¥æ–‡å": jp_name, "æˆåˆ†è‹±æ–‡å": en_name, "ä¾†æº": source
            })
            bar.progress((i + 1) / len(df))
            
        final_df = pd.DataFrame(results)
        st.success("ğŸ‰ å…¨é‡å°ç…§å®Œæˆï¼")
        st.dataframe(final_df, use_container_width=True)
        
        # ä¸‹è¼‰æˆæœ
        out = io.BytesIO()
        with pd.ExcelWriter(out, engine='xlsxwriter') as writer:
            final_df.to_excel(writer, index=False)
        st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´ Excel å ±å‘Š", out.getvalue(), "Medicine_Full_Report.xlsx")
